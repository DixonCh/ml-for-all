{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network Multi-layer perceptron\n",
    "\n",
    "A feed-forward bachpropagation multilayer perceptron neural network using TensorFlow to classify Nepali handwritten digits.\n",
    "\n",
    "- Author: Ashok Kuamr Pant\n",
    "- Company: Treeleaf Technologies Pvt. Ltd. (treeleaf.ai)\n",
    "- Email: asokpant@gmail.com\n",
    "- Date: 11 May, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nepali Handwritten Character Dataset Overview\n",
    "### Overview\n",
    "NHCD dataset is a Nepali handwritten character dataset. It contains three individual categories (numerals, vowels and consonants). Samples are collected from 40 individuals from different fields and cropped for character boundary and resized to 28x28.\n",
    "\n",
    "* Numerals (288 samples per class, 10 classes)\n",
    "* Vowels (221 samples per class, 12 classes)\n",
    "* Consonants (205 samples per class, 36 classes)\n",
    "\n",
    "### Samples\n",
    "* Numerals ![numerals.png](../../data/numerals.png)\n",
    "* Vowels ![vowels.png](../../data/vowels.png)\n",
    "* Consonants ![consonants.png](../../data/consonants.png)\n",
    "    \n",
    "### Citation\n",
    "Please cite in your publications if it helps your research:\n",
    "\n",
    "    ```text\n",
    "    @inproceedings{pant2012off,\n",
    "      title={Off-line Nepali handwritten character recognition using Multilayer Perceptron and Radial Basis Function neural networks},\n",
    "      author={Pant, Ashok Kumar and Panday, Sanjeeb Prasad and Joshi, Shashidhar Ram},\n",
    "      booktitle={2012 Third Asian Himalayas International Conference on Internet},\n",
    "      pages={1--5},\n",
    "      year={2012},\n",
    "      organization={IEEE}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "More info: https://www.kaggle.com/ashokpant/devanagari-character-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "from utils.dataset import create_handwritten_dataset\n",
    "\n",
    "train_data, train_labels, test_data, test_labels, label_map = create_handwritten_dataset(\n",
    "        \"/home/ashok/Data/Datasets/devanagari-character-dataset/nhcd/numerals\", test_ratio=0.2)\n",
    "\n",
    "n_classes = len(label_map)\n",
    "image_size = (28, 28)\n",
    "image_channel = 1\n",
    "n_train_samples = len(train_labels)\n",
    "n_test_samples = len(test_labels)\n",
    "\n",
    "print(\"Classes: {}, Label map: {}\".format(n_classes, label_map))\n",
    "print(\"Train samples: {}, Test samples: {}\".format(n_train_samples, n_test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "hidden_layer_neurons=[1024,512]\n",
    "n_input = image_size[0]*image_size[1]*image_channel# 28*25*1=784\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "\n",
    "# Create model\n",
    "def layer(inputs, n_neurons, activation=\"linear\", name=None):\n",
    "    \"\"\"\n",
    "    activation: [relu, sigmoid, tenh, linear]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        n_inputs = int(inputs.get_shape()[1])\n",
    "        w = tf.Variable(tf.random_normal([n_inputs, n_neurons]), name=name+\"_w\")\n",
    "        b = tf.Variable(tf.random_normal([n_neurons]), name=name+\"_b\")\n",
    "        output = tf.add(tf.matmul(inputs, w), b)\n",
    "        \n",
    "        if activation ==\"relu\":\n",
    "            ouput = tf.nn.relu(output)\n",
    "        elif activation == \"sigmoid\":\n",
    "            ouput = tf.nn.sigmoid(output)\n",
    "        elif activation == \"tenh\":\n",
    "            ouput = tf.nn.tenh(output)\n",
    "        elif activation == \"linear\":\n",
    "            pass\n",
    "        else:\n",
    "            raise(\"Unknown activation, {}\".format(activation))\n",
    "            \n",
    "        return output\n",
    "    \n",
    "def multilayer_perceptron(inputs, hidden_layer_neurons, n_classes):\n",
    "    if hidden_layer_neurons is None or len(hidden_layer_neurons) ==0: #Perceptron\n",
    "        raise(\"Please provide hidden layers. hidden_layer_neurons = [N,O,P,...]\")\n",
    "    net = inputs\n",
    "    for i in range(len(hidden_layer_neurons)):\n",
    "        net = layer(net, hidden_layer_neurons[i], activation=\"relu\", name=\"hidden_layer\"+str(i+1))\n",
    "    net = layer(net, n_classes, \"linear\", \"output_layer\")\n",
    "    return net\n",
    "    \n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, hidden_layer_neurons,  n_classes)\n",
    "\n",
    "# Predictions\n",
    "pred_probas = tf.nn.softmax(pred,name=\"pred_prob\")\n",
    "pred_classes = tf.argmax(pred, axis=1, name=\"pred_class\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "acc =  tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Summary\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(init)\n",
    "summary_writer = tf.summary.FileWriter('/tmp/mlp', sess.graph)\n",
    "\n",
    "# Training cycle\n",
    "global_step = 0\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(n_train_samples/batch_size)\n",
    "    if n_train_samples % batch_size != 0: # samller last batch\n",
    "        total_batch += 1\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        start = i*batch_size\n",
    "        end = start+batch_size\n",
    "        if end > n_train_samples:\n",
    "            end = n_train_samples-1\n",
    "        batch_x = train_data[start:end]\n",
    "        batch_y = train_labels[start:end]\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c, summary = sess.run([optimizer, cost, summary_op], feed_dict={x: batch_x,\n",
    "                                                      y: batch_y})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "        summary_writer.add_summary(summary, global_step)\n",
    "        global_step += 1\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch: {:04d}, cost = {:.9f}\".format(epoch+1, avg_cost))\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "train_acc = sess.run(acc, feed_dict={x:train_data, y: train_labels})\n",
    "test_acc = sess.run(acc, feed_dict={x:test_data, y: test_labels})\n",
    "\n",
    "print(\"Train Accuracy: {:.2f}%\".format(train_acc*100))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_acc*100))\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.dataset import imread, imshow, imshow_array, imresize, normalize_array, im2bw, pil2array, rgb2gray\n",
    "\n",
    "image = imread('/home/ashok/Projects/ml-for-all-github/data/five.png')\n",
    "\n",
    "imshow(image)\n",
    "\n",
    "if image.size != image_size:\n",
    "    image = imresize(image, image_size)\n",
    "    \n",
    "image = rgb2gray(image)\n",
    "image  = pil2array(image)\n",
    "image = normalize_array(image)\n",
    "image = np.reshape(image, (image_size[0]*image_size[1]*image_channel))\n",
    "image  = np.reshape(np.asarray(image), image_size[0]*image_size[1]*image_channel)\n",
    "\n",
    "output  = sess.run(pred_probas, feed_dict={x:[image]})\n",
    "output_label = np.argmax(output)\n",
    "\n",
    "print('Output label: {}, score: {:.2f}%'.format(output_label, output[0][output_label]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
